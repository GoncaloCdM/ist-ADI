{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 5: Reinforcement learning\n",
    "\n",
    "In the end of the lab, you should submit all code/answers written in the tasks marked as \"Activity n. XXX\", together with the corresponding outputs and any replies to specific questions posed to the e-mail <adi.tecnico@gmail.com>. Make sure that the subject is of the form [&lt;group n.&gt;] LAB &lt;lab n.&gt;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The puddle world domain\n",
    "\n",
    "Consider the puddleworld domain from the homework, depicted in the figure below.\n",
    "\n",
    "<img src=\"puddleworld.png\" width=\"400px\">\n",
    "\n",
    "In it, an all terrain vehicle must navigate a 20 &times; 20 gridworld. The three shaded cells in the upper right corner correspond to the goal state, while the L-shaped shaded cells in the middle of the grid correspond to a puddle in which the vehicle may get stuck and damaged. \n",
    "\n",
    "The vehicle has available the standard four actions, _up_, _down_, _left_ and _right_. Each action\n",
    "\n",
    "* Succeeds and moves the vehicle to the adjacent cell in the corresponding direction with a probability of $0.92$; \n",
    "* Fails and moves the vehicle to any of the other 3 adjacent cells with a probability of $0.2$; \n",
    "* Fails and the vehicle remains in the same cell with a probability of $0.2$.\n",
    "\n",
    "The vehicle incurs maximal cost ($1$) for standing in the darker part of the puddle; in the lighter part of the puddle, it incurs a cost of $0.5$. Each movement costs $0.05$ and the goal cells cost $0$.\n",
    "\n",
    "The problem can be described as an MDP $(\\mathcal{X},\\mathcal{A},\\mathbf{P},c,\\gamma)$ as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Problem specific parameters\n",
    "GRIDSIZE = 20\n",
    "\n",
    "def puddlecost(x):\n",
    "    ''' puddlecost : int -> int\n",
    "\n",
    "        puddlecost(x) returns the cost for the puddle area corresponding to state x:\n",
    "        * if x is in the dark puddle area, it returns 1;\n",
    "        * if x is in the light puddle area, it returns 0.5;\n",
    "        * otherwise, it returns 0.\n",
    "        \n",
    "    '''\n",
    "\n",
    "    i = x // GRIDSIZE\n",
    "    j = x % GRIDSIZE\n",
    "    \n",
    "    if (j in (4, 5) and i in range(1, 8)) or \\\n",
    "       (i in (8, 9) and j in range(3, 13)):\n",
    "        return 1.0\n",
    "    \n",
    "    if (j in range(3, 7) and i in range(0, 9)) or \\\n",
    "       (i in range(7, 11) and j in range(2, 14)):\n",
    "        return 0.5\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# -- End: puddlecost\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=10)\n",
    "\n",
    "\n",
    "# States\n",
    "X = [(i, j) for i in range(GRIDSIZE) for j in range(GRIDSIZE)]\n",
    "nX = len(X)\n",
    "\n",
    "# Actions\n",
    "A = ['U', 'D', 'L', 'R']\n",
    "nA = len(A)\n",
    "\n",
    "# Transition probabilities and cost\n",
    "Pu = np.zeros((nX, nX))\n",
    "Pd = np.zeros((nX, nX))\n",
    "Pl = np.zeros((nX, nX))\n",
    "Pr = np.zeros((nX, nX))\n",
    "\n",
    "c = np.zeros((nX, nA))\n",
    "\n",
    "for x in range(nX):\n",
    "    (i, j) = X[x]\n",
    "    \n",
    "    xu = X.index((i, max(j - 1, 0)))\n",
    "    xd = X.index((i, min(j + 1, GRIDSIZE - 1)))\n",
    "    xl = X.index((max(i - 1, 0), j))\n",
    "    xr = X.index((min(i + 1, GRIDSIZE - 1), j))\n",
    "    \n",
    "    # Successfull transition\n",
    "    Pu[x, xu] += 0.92\n",
    "    Pd[x, xd] += 0.92\n",
    "    Pl[x, xl] += 0.92\n",
    "    Pr[x, xr] += 0.92\n",
    "\n",
    "    # Failed transition (stays in place)\n",
    "    Pu[x, x] += 0.02\n",
    "    Pd[x, x] += 0.02\n",
    "    Pl[x, x] += 0.02\n",
    "    Pr[x, x] += 0.02\n",
    "\n",
    "    # Failed transition (oposite direction)\n",
    "    Pu[x, xd] += 0.02\n",
    "    Pd[x, xu] += 0.02\n",
    "    Pl[x, xr] += 0.02\n",
    "    Pr[x, xl] += 0.02\n",
    "\n",
    "    # Failed transition (sideways)\n",
    "    Pu[x, xl] += 0.02\n",
    "    Pu[x, xr] += 0.02\n",
    "    Pd[x, xl] += 0.02\n",
    "    Pd[x, xr] += 0.02\n",
    "    Pl[x, xu] += 0.02\n",
    "    Pl[x, xd] += 0.02\n",
    "    Pr[x, xu] += 0.02\n",
    "    Pr[x, xd] += 0.02\n",
    "    \n",
    "    if x not in (GRIDSIZE * (GRIDSIZE - 2), GRIDSIZE * (GRIDSIZE - 1), GRIDSIZE * (GRIDSIZE - 1) + 1):\n",
    "        c[x, :] = min(0.05 + puddlecost(x), 1)\n",
    "    \n",
    "P = [Pu, Pd, Pl, Pr]\n",
    "\n",
    "# Discount\n",
    "gamma = 0.95\n",
    "\n",
    "# Observe cost function\n",
    "plt.figure()\n",
    "plt.imshow(c[:, 0].reshape(GRIDSIZE, GRIDSIZE).T, cmap='Greys', origin='upper')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Compute the optimal $Q$-function for the MDP defined above using value iteration. As your stopping condition, use an error between iterations smaller than `1e-8`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.        \n",
    "\n",
    "Write down a Python function that, given a Q-function $Q$ and a state $x$, selects a random action using the $\\epsilon$-greedy policy obtained from $Q$ for state $x$. Your function should receive an optional parameter, corresponding to $\\epsilon$, with default value of 0.1. \n",
    "\n",
    "**Note:** In the case of two actions with the same value, your $\\epsilon$-greedy policy should randomize between the two.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Model-based learning\n",
    "\n",
    "You will now run the model-based learning algorithm discussed in class, and evaluate its learning performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 3.        \n",
    "\n",
    "Run the model-based reinforcement learning algorithm discussed in class to compute $Q^*$ for $500,000$ iterations. Initialize each transition probability matrix as the identity and the cost function as all-zeros. Use an $\\epsilon$-greedy policy with $\\epsilon=0.1$ (use the function from Activity 2). Note that, at each step,\n",
    "\n",
    "* You will need to select an action according to the $\\epsilon$-greedy policy;\n",
    "* The state and action, you will then compute the cost and generate the next state; \n",
    "* With this transition information (state, action, cost, next-state), you can now perform an update. \n",
    "* When updating the components $(x,a)$ of the model, use the step-size\n",
    "\n",
    "$$\\alpha_t=\\frac{1}{N_t(x,a)+1},$$\n",
    "\n",
    "where $N_t(x,a)$ is the number of visits to the pair $(x,a)$ up to time step $t$.\n",
    "\n",
    "In order to ensure that your algorithm visits every state and action a sufficient number of times, after the boat reaches the goal cell, make one further step, the corresponding update, and then reset the position of the vehicle to a random state in the environment.\n",
    "\n",
    "Plot the norm $\\|Q^*-Q^{(k)}\\|$ every 500 iterations of your method, where $Q^*$ is the optimal $Q$-function computed in Activity 1.\n",
    "\n",
    "**Note:** The simulation may take a bit. Don't despair.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Temporal-difference learning\n",
    "\n",
    "You will now run both Q-learning and SARSA, and compare their learning performance with that of the model-based method just studied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 4.        \n",
    "\n",
    "Repeat Activity 3 but using the $Q$-learning algorithm with a learning rate $\\alpha=0.3$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 5.\n",
    "\n",
    "Repeat Activity 4 but using the SARSA algorithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 6.\n",
    "\n",
    "Discuss the differences observed between the performance of the three methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your comments here."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
